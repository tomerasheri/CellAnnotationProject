{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torchvision.datasets\nfrom torch.utils.data import Dataset\nimport torch\nimport pandas as pd\nimport os\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom skimage import io, transform\nimport torchvision.transforms as transforms\nimport numbers","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2021-10-29T22:23:40.519875Z","iopub.execute_input":"2021-10-29T22:23:40.520161Z","iopub.status.idle":"2021-10-29T22:23:40.529857Z","shell.execute_reply.started":"2021-10-29T22:23:40.520131Z","shell.execute_reply":"2021-10-29T22:23:40.528926Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"code","source":"annot_root = \"../input/sartorius-cell-instance-segmentation/train\"\nimages_only_root = \"../input/sartorius-cell-instance-segmentation/train\"\ntest_root = \"Data/test\"\ntrain_csv = \"../input/sartorius-cell-instance-segmentation/train.csv\"\nepsilon = np.finfo(float).eps\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 16\ntorch.cuda.empty_cache()\ndevice","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-29T22:23:40.532022Z","iopub.execute_input":"2021-10-29T22:23:40.532213Z","iopub.status.idle":"2021-10-29T22:23:40.626154Z","shell.execute_reply.started":"2021-10-29T22:23:40.532189Z","shell.execute_reply":"2021-10-29T22:23:40.625428Z"},"trusted":true},"execution_count":262,"outputs":[]},{"cell_type":"code","source":"def compare_images(image, annotation, title):\n    fig = plt.figure(figsize=(15,5))\n    ax = fig.subplots(ncols=2)\n    ax[0].imshow(formatForPlot(image), cmap='gray')\n    ax[1].imshow(formatForPlot(annotation), cmap='gray')\n    ax[0].set_title(\"image\")\n    ax[1].set_title(\"annotation\")\n    fig.suptitle(title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T22:23:40.628349Z","iopub.execute_input":"2021-10-29T22:23:40.629223Z","iopub.status.idle":"2021-10-29T22:23:40.636427Z","shell.execute_reply.started":"2021-10-29T22:23:40.629175Z","shell.execute_reply":"2021-10-29T22:23:40.635750Z"},"trusted":true},"execution_count":263,"outputs":[]},{"cell_type":"code","source":"def process_df(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.set_index('id')\n    annotations = df.groupby(['id'])['annotation'].transform(\n        lambda x: ' '.join(x)).drop_duplicates()\n    types = df.groupby(['id'])['cell_type'].unique()\n    return annotations.to_frame().join(types)\n\ndef reconstruct_annotations_image(annotations, height, width) -> np.array:\n    annotations = np.array(annotations.split(\" \")).astype(int)\n    image = np.zeros(shape=(height * width))\n    for i in np.arange(0, len(annotations), 2):\n        begin = annotations[i]\n        length = annotations[i+1] - 1\n        image[begin: begin+length] = 1\n    return image.reshape(height, width)\n\nclass CellsAnnDataSet(Dataset):\n    def __init__(self, root_dir, csv_file, color_transform=None, shape_transform=None):\n        self.df = None\n        df = pd.read_csv(csv_file)\n        self.df = process_df(df)\n        self.root_dir = root_dir\n        self.color_transform = color_transform\n        self.shape_transform = shape_transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        if torch.is_tensor(index):\n            index = index.tolist()\n        image_id = self.df.iloc[index].name\n        image_name = f\"{image_id}.png\"\n        image_path = os.path.join(self.root_dir, image_name)\n        image = plt.imread(image_path) / 255\n        width, height = image.shape[:2]\n        annotations = self.df.iloc[index]['annotation']\n        annotation = reconstruct_annotations_image(annotations, width, height)\n        sample = {'image': image, \"annotation\": annotation}\n        if self.color_transform is not None:\n            sample['image'] = self.color_transform(sample['image'])\n        if self.shape_transform is not None:\n            sample = self.shape_transform(sample)\n        return sample\n\nclass CellsImagesOnlyDataSet(Dataset):\n    def __init__(self, root_dir, transform = None):\n        self.images = glob.glob(root_dir + \"/*.png\")\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        if torch.is_tensor(index):\n            index = index.tolist()\n        image = plt.imread(self.images[index]) / 255\n        if self.transform is not None:\n            image = self.transform(image)\n        return image","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-29T22:23:40.638637Z","iopub.execute_input":"2021-10-29T22:23:40.638939Z","iopub.status.idle":"2021-10-29T22:23:40.656870Z","shell.execute_reply.started":"2021-10-29T22:23:40.638903Z","shell.execute_reply":"2021-10-29T22:23:40.656034Z"},"trusted":true},"execution_count":264,"outputs":[]},{"cell_type":"code","source":"def assertSizeArgValid(arg, is_four_tuple_valid=False):\n    assert isinstance(arg, (int, tuple))\n    if isinstance(arg, int):\n        if not is_four_tuple_valid:\n            return arg,arg\n        return arg,arg,arg,arg\n    else:\n        assert len(arg) == 2 or (len(arg) == 4 and is_four_tuple_valid)\n        if len(arg) == 2:\n            if not is_four_tuple_valid:\n                return arg\n            return arg + arg\n        else:\n            return arg\n\nclass RandomCrop(object):\n    def __call__(self, sample):\n        if not isinstance(sample, dict):\n            sample = sample.numpy().squeeze()\n            images = [sample]\n        else:\n            image, annotation = sample['image'], sample['annotation']\n            images = [image, annotation]\n        h, w = images[0].shape[:2]\n        new_h, new_w = np.random.randint(256, 510, 2)\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n        for i in range(len(images)):\n            images[i] = images[i][top: top + new_h, left: left + new_w]\n        if len(images) == 1:\n            return ToTensor()(images[0])\n        image, annotation = images\n        return {'image': image, 'annotation': annotation}\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        if not isinstance(sample, dict):\n            return transforms.ToTensor()(sample[:,:,np.newaxis])\n        image, annotation = sample['image'], sample['annotation']\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C x H x W\n        return {'image': transforms.ToTensor()(image[:,:,np.newaxis]),\n                'annotation': transforms.ToTensor()(annotation[:,:,np.newaxis])}\n\nclass RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, sample):\n        if not isinstance(sample, dict):\n            return transforms.RandomHorizontalFlip(self.p)(sample)\n        image, annotation = sample['image'], sample['annotation']\n        if torch.rand(1) < self.p:\n            image = np.flip(image, 1)\n            annotation = np.flip(annotation, 1)\n        return {'image': image, 'annotation': annotation}\n\nclass RandomVerticalFlip(object):\n    def __init__(self, p=0.5):\n        assert isinstance(p, (int, float))\n        assert 0 <= p <= 1\n        self.p = p\n    def __call__(self, sample):\n        if not isinstance(sample, dict):\n            return transforms.RandomVerticalFlip(self.p)(sample)\n        image, annotation = sample['image'], sample['annotation']\n        if torch.rand(1) < self.p:\n            image = np.flip(image, 0)\n            annotation = np.flip(annotation, 0)\n        return {'image': image, 'annotation': annotation}\n\nclass Normalize(object):\n    def __init__(self, mean, std, inplace=False):\n        self.mean = mean\n        self.std = std\n        self.inplace = inplace\n\n    def __call__(self, sample):\n        if not isinstance(sample, dict):\n            return transforms.Normalize(self.mean, self.std, self.inplace)(sample)\n        image, annotation = sample['image'], sample['annotation']\n        image = (image - self.mean) / self.std\n        annotation = (annotation - self.mean) / self.std\n        return {'image': image, 'annotation': annotation}\n\nclass RandomPadToSize(object):\n    def __init__(self, output_size, fill=0):\n        self.output_size = assertSizeArgValid(output_size)\n        self.fill = fill\n\n    def __call__(self, sample):\n        input_images, output_images = [], []\n        if not isinstance(sample, dict):\n            input_images = [sample.numpy().squeeze()]\n        else:\n            input_images = [sample['image'], sample['annotation']]\n        h, w = input_images[0].shape[:2]\n        new_h, new_w = self.output_size\n        assert new_w > w\n        assert new_h > h\n        top = np.random.randint(0, new_h - h)\n        left = np.random.randint(0, new_w - w)\n        for input_image in input_images:\n            output_image = np.full(self.output_size, self.fill, dtype=np.float64)\n            output_image[top:top + h, left:left + w] = input_image\n            output_images.append(output_image)\n        if len(output_images) == 1:\n            return ToTensor()(output_images[0])\n        else:\n            image, annotation = output_images\n            return {'image': image, 'annotation': annotation}\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-29T22:23:40.658759Z","iopub.execute_input":"2021-10-29T22:23:40.659227Z","iopub.status.idle":"2021-10-29T22:23:40.689259Z","shell.execute_reply.started":"2021-10-29T22:23:40.659183Z","shell.execute_reply":"2021-10-29T22:23:40.688469Z"},"trusted":true},"execution_count":265,"outputs":[]},{"cell_type":"code","source":"shape_transform = [\n    RandomCrop(),\n    RandomHorizontalFlip(),\n    RandomVerticalFlip(),\n    RandomPadToSize((520, 704)),\n    Normalize(0.5, 0.5)\n]\nimage_w_ann_data = CellsAnnDataSet(annot_root, train_csv,\n                                   shape_transform=transforms.Compose(shape_transform + [ToTensor()]))\nimage_only_data = CellsImagesOnlyDataSet(images_only_root,\n                                         transform=transforms.Compose([ToTensor()] + shape_transform))\nimages_w_ann_loader = torch.utils.data.DataLoader(image_w_ann_data, batch_size = batch_size, shuffle=True)\nimages_only_loader = torch.utils.data.DataLoader(image_only_data, batch_size = batch_size, shuffle=True)\n\nsample = image_w_ann_data[0]\nsample_image, sample_ann = sample['image'], sample['annotation']","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-29T22:23:40.691657Z","iopub.execute_input":"2021-10-29T22:23:40.691917Z","iopub.status.idle":"2021-10-29T22:23:41.171708Z","shell.execute_reply.started":"2021-10-29T22:23:40.691883Z","shell.execute_reply":"2021-10-29T22:23:41.170986Z"},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"code","source":"def formatForPlot(*args):\n    outputs = []\n    for arg in args:\n        if torch.is_tensor(arg):\n            arg =  arg.detach().cpu().numpy()\n        outputs.append(arg.squeeze())\n    return outputs if len(outputs) > 1 else outputs[0] if len(outputs) > 0 else None\n\ndef plot_generator(orig_image, orig_ann, fake_ann, d_losses, g_losses):\n    orig_image, orig_ann, fake_ann = formatForPlot(orig_image, orig_ann, fake_ann)\n    fig = plt.figure(figsize=(15,5))\n    ax = fig.subplots(ncols=4)\n    ax[0].imshow(orig_image.squeeze(), cmap='gray')\n    ax[1].imshow(orig_ann.squeeze(), cmap='gray')\n    ax[2].imshow(fake_ann.squeeze(), cmap='gray')\n    ax[3].plot(np.arange(1, len(d_losses) + 1), d_losses, label=\"D loss\")\n    ax[3].plot(np.arange(1, len(g_losses) + 1), g_losses, label=\"G loss\")\n    ax[3].legend()\n    ax[0].set_title(\"image\")\n    ax[1].set_title(\"real annotation\")\n    ax[2].set_title(\"generated annotation\")\n    ax[3].set_title(\"loss plot\")\n    fig.subplots_adjust(bottom=0.26, top=0.74)\n    plt.show()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-29T22:23:41.173167Z","iopub.execute_input":"2021-10-29T22:23:41.173420Z","iopub.status.idle":"2021-10-29T22:23:41.185300Z","shell.execute_reply.started":"2021-10-29T22:23:41.173384Z","shell.execute_reply":"2021-10-29T22:23:41.184579Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"# draw some images:\nfor i in range(5):\n    sample = image_w_ann_data[i]\n    image = sample['image']\n    annotation = sample['annotation']\n    image_only = image_only_data[i]\n    fig = plt.figure(figsize=(15,5))\n    ax = fig.subplots(ncols=3)\n    ax[0].imshow(formatForPlot(image), cmap='gray')\n    ax[1].imshow(formatForPlot(annotation), cmap='gray')\n    ax[2].imshow(formatForPlot(image_only), cmap='gray')\n    ax[0].set_title(\"image\")\n    ax[1].set_title(\"annotation\")\n    ax[2].set_title(\"only_image\")\n    plt.show()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-29T22:23:41.186928Z","iopub.execute_input":"2021-10-29T22:23:41.187221Z","iopub.status.idle":"2021-10-29T22:23:44.051510Z","shell.execute_reply.started":"2021-10-29T22:23:41.187183Z","shell.execute_reply":"2021-10-29T22:23:44.050812Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"code","source":"def init_normal_weights(m, mean=0, std=0.02):\n    if isinstance(m, nn.Conv2d):\n        torch.nn.init.normal_(m.weight, mean=mean, std=std)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.conv1 = nn.Conv2d(2, 64, kernel_size=(4,4), stride=(2,2))\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=(4,4), stride=(2,2))\n        self.batchNorm2 = nn.BatchNorm2d(128)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4,4), stride=(2,2))\n        self.batchNorm3 = nn.BatchNorm2d(256)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=(4,4), stride=(2,2))\n        self.batchNorm4 = nn.BatchNorm2d(512)\n        self.conv5 = nn.Conv2d(512, 512, kernel_size=(4,4), stride=(2,2))\n        self.batchNorm5 = nn.BatchNorm2d(512)\n        self.conv6 = nn.Conv2d(512, 1, kernel_size=(4,4), stride=(2,2))\n        self.fc = nn.Linear(6*9, 1)\n        for conv in [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.conv6]:\n            init_normal_weights(conv)\n\n\n    def forward(self, x, y):\n        x = x.float().to(device)\n        y = y.float().to(device)\n        z = torch.cat((x, y), dim=1)\n        z = F.leaky_relu(self.conv1(z), negative_slope=0.2, inplace=True)\n        z = F.leaky_relu(self.batchNorm2(self.conv2(z)), negative_slope=0.2, inplace=True)\n        z = F.leaky_relu(self.batchNorm3(self.conv3(z)), negative_slope=0.2, inplace=True)\n        z = F.leaky_relu(self.batchNorm4(self.conv4(z)), negative_slope=0.2, inplace=True)\n        z = F.leaky_relu(self.batchNorm5(self.conv5(z)), negative_slope=0.2, inplace=True)\n        z = self.conv6(z)\n        z = torch.flatten(z,1)\n        z = self.fc(z)\n        return torch.sigmoid(z)\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, batchNorm=True):\n        super(EncoderBlock, self).__init__()\n        layers = [nn.Conv2d(in_ch, out_ch, kernel_size=5, stride=2)]\n        if batchNorm:\n            layers.append(nn.BatchNorm2d(out_ch))\n        layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n        init_normal_weights(layers[0])\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\nkernel_size = (5,5)\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, dropout = True, output_padding=(0,0)):\n        super(DecoderBlock, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=kernel_size, stride=(2,2),\n                               output_padding=output_padding),\n            nn.BatchNorm2d(out_ch)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(0.5))\n        init_normal_weights(layers[0])\n        self.layers = nn.Sequential(*layers)\n        self.conv1on1 = nn.Conv2d(out_ch*2, out_ch, kernel_size=(1,1))\n\n    def forward(self, x, skip_in):\n        x = self.layers(x)\n        x = torch.cat((x, skip_in), dim=1)\n        return F.relu(self.conv1on1(x))\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.e1 = EncoderBlock(1, 64, batchNorm=False)\n        self.e2 = EncoderBlock(64, 128)\n        self.e3 = EncoderBlock(128, 256)\n        self.e4 = EncoderBlock(256, 512)\n        self.e5 = EncoderBlock(512, 512)\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=kernel_size, stride=(2,2)),\n            nn.ReLU(inplace=True)\n        )\n        init_normal_weights(self.bottleneck[0])\n        self.d1 = DecoderBlock(512, 512)\n        self.d2 = DecoderBlock(512, 512, dropout=False)\n        self.d3 = DecoderBlock(512, 256, dropout=False, output_padding=(1,0))\n        self.d4 = DecoderBlock(256, 128, dropout=False)\n        self.d5 = DecoderBlock(128, 64, dropout=False, output_padding=(1,1))\n        self.output = nn.Sequential(\n            nn.ConvTranspose2d(64, 1, kernel_size=kernel_size, stride=(2,2),\n                               output_padding=(1,1)),\n            nn.Tanh()\n        )\n        init_normal_weights(self.output[0])\n\n    def forward(self, x):\n        x = x.float().to(device)\n        # encoding:\n        e1 = self.e1(x)\n        e2 = self.e2(e1)\n        e3 = self.e3(e2)\n        e4 = self.e4(e3)\n        e5 = self.e5(e4)\n\n        # bottleneck:\n        b = self.bottleneck(e5)\n\n        # decoding:\n        d1 = self.d1(b, e5)\n        d2 = self.d2(d1, e4)\n        d3 = self.d3(d2, e3)\n        d4 = self.d4(d3, e2)\n        d5 = self.d5(d4, e1)\n        return self.output(d5)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-29T22:23:44.052835Z","iopub.execute_input":"2021-10-29T22:23:44.053203Z","iopub.status.idle":"2021-10-29T22:23:44.083877Z","shell.execute_reply.started":"2021-10-29T22:23:44.053165Z","shell.execute_reply":"2021-10-29T22:23:44.083092Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"def minmax_loss(D_fake, D_real  = None):\n    genLoss = 0.5 * torch.mean(torch.log(1-D_fake + epsilon))\n    if D_real is not None:\n        return -0.5 * torch.mean(torch.log(D_real+epsilon)) - genLoss\n    return genLoss\n\ndef non_saturating_loss(D_fake, D_real  = None):\n    if D_real is not None:\n        return -0.5 * torch.mean(torch.log(D_real + epsilon)) - 0.5 * torch.mean(torch.log(1-D_fake + epsilon))\n    return -0.5 * torch.mean(torch.log(D_fake + epsilon))\n\ndef least_squares_loss(D_fake, D_real  = None):\n    if D_real is not None:\n        return (0.5 * torch.mean((D_real - 1) ** 2)) + (0.5 * torch.mean(D_fake ** 2))\n    return 0.5 * torch.mean((D_fake - 1) ** 2)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T22:23:44.085091Z","iopub.execute_input":"2021-10-29T22:23:44.085331Z","iopub.status.idle":"2021-10-29T22:23:44.098793Z","shell.execute_reply.started":"2021-10-29T22:23:44.085297Z","shell.execute_reply":"2021-10-29T22:23:44.098077Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"class CellGAN(nn.Module):\n    def __init__(self):\n        super(CellGAN, self).__init__()\n        self.D = Discriminator()\n        self.G = Generator()\n        self.to(device)\n\n    def forward(self, x):\n        self.G.eval()\n        with torch.no_grad():\n            return self.G(x)\n\n\n    def G_loss(self, image):\n        self.G.train()\n        self.D.eval()\n        fake_ann = self.G(image)\n        D_fake = self.D(image, fake_ann)\n#         return -torch.mean(torch.log(D_fake + epsilon))\n        return non_saturating_loss(D_fake)\n\n    def D_loss(self, image, real_ann = None):\n        self.D.train()\n        self.G.eval()\n        fake_ann = self.G(image)\n        D_fake = self.D(image, fake_ann)\n        if real_ann is not None:\n            D_real = self.D(image, real_ann)\n#             return -torch.mean(torch.log(D_real + epsilon) + torch.log(1-D_fake + epsilon))\n            return non_saturating_loss(D_fake, D_real)\n        else:\n#             return -torch.mean(torch.log(1-D_fake + epsilon))\n            return non_saturating_loss(D_fake)\n\n        \n    def run_training(self,epochs: int = 10, D_pretrain = 3, G_lr: int = 2e-6, D_lr: int = 2e-3, plot:bool = True):\n        D_Opt = torch.optim.Adam(params=self.D.parameters(), lr=D_lr)\n        G_Opt = torch.optim.Adam(params=self.G.parameters(), lr=G_lr)\n        d_losses, g_losses = [], []\n        d_running_loss, g_running_loss = 0,0\n        \n        # pretrain the discriminator:\n        for pretrain_epoch in range(1, D_pretrain+1):\n            print(f\"pre-training discriminator, epoch {pretrain_epoch}\")\n            for batch in images_w_ann_loader:\n                image, ann = batch['image'].to(device), batch['annotation'].to(device)\n                d_loss = self.D_loss(image, ann)\n                D_Opt.zero_grad()\n                d_loss.backward()\n                D_Opt.step()\n\n                # semi supervised:\n                for _ in range(3):\n                    image = next(iter(images_only_loader))\n                    d_loss = self.D_loss(image)\n                    D_Opt.zero_grad()\n                    d_loss.backward()\n                    D_Opt.step()\n        \n        print (f\"Finished pretrain of {D_pretrain} epochs\")\n        for epoch in range(1, epochs+1):\n            d_epoch_losses, g_epoch_losses = [], []\n            for i, batch in enumerate(images_w_ann_loader, 0):\n                image, ann = batch['image'].to(device), batch['annotation'].to(device)\n                d_step_loss, g_step_loss = 0, 0\n                # train discriminator:\n                d_loss = self.D_loss(image, ann)\n                D_Opt.zero_grad()\n                d_loss.backward()\n                D_Opt.step()\n                d_step_loss += d_loss.item() / 2\n\n                # train generator:\n                g_loss = self.G_loss(image)\n                G_Opt.zero_grad()\n                g_loss.backward()\n                G_Opt.step()\n                g_step_loss += g_loss.item() / 2\n\n                # semi supervised:\n                for _ in range(3):\n                    step_d_losses, step_g_losses = [],[]\n                    image = next(iter(images_only_loader))\n\n                    # train discriminator:\n                    d_loss = self.D_loss(image)\n                    D_Opt.zero_grad()\n                    d_loss.backward()\n                    D_Opt.step()\n                    g_step_loss += d_loss.item() / 6\n\n                    # train generator:\n                    g_loss = self.G_loss(image)\n                    G_Opt.zero_grad()\n                    g_loss.backward()\n                    G_Opt.step()\n                    g_step_loss += g_loss.item() / 6\n                d_epoch_losses.append(d_step_loss)\n                g_epoch_losses.append(g_step_loss)\n                d_running_loss += d_step_loss\n                g_running_loss += g_step_loss\n                \n                if i%5 == 4:\n                    print(f'epoch [{epoch}/{epochs}], step[{i+1}/38],'\n                    f' generator loss:{round(g_running_loss / 5, 3)},'\n                    f' discriminator loss:{round(d_running_loss / 5, 3)}')\n                    d_running_loss, g_running_loss = 0, 0\n\n            d_losses.append(np.mean(d_epoch_losses))\n            g_losses.append(np.mean(g_epoch_losses))\n\n            if plot:\n                self.G.eval()\n                with torch.no_grad():\n                    fake_ann = self.G(sample_image[None, :, :, :])\n                    plot_generator(sample_image, sample_ann, fake_ann, d_losses, g_losses)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-29T22:23:44.102465Z","iopub.execute_input":"2021-10-29T22:23:44.102739Z","iopub.status.idle":"2021-10-29T22:23:44.126066Z","shell.execute_reply.started":"2021-10-29T22:23:44.102696Z","shell.execute_reply":"2021-10-29T22:23:44.125250Z"},"trusted":true},"execution_count":271,"outputs":[]},{"cell_type":"code","source":"gan = CellGAN()\ngan.run_training()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T22:23:44.127339Z","iopub.execute_input":"2021-10-29T22:23:44.128035Z"},"trusted":true},"execution_count":null,"outputs":[]}]}