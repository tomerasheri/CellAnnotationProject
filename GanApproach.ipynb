{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision.datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from skimage import io, transform\n",
    "import torchvision.transforms as transforms\n",
    "import numbers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annot_root = \"Data/train\"\n",
    "images_only_root = \"Data/train_semi_supervised/subfolder\"\n",
    "test_root = \"Data/test\"\n",
    "train_csv = \"Data/train.csv\"\n",
    "epsilon = np.finfo(float).eps\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.set_index('id')\n",
    "    annotations = df.groupby(['id'])['annotation'].transform(\n",
    "        lambda x: ' '.join(x)).drop_duplicates()\n",
    "    types = df.groupby(['id'])['cell_type'].unique()\n",
    "    return annotations.to_frame().join(types)\n",
    "\n",
    "def reconstruct_annotations_image(annotations, height, width) -> np.array:\n",
    "    annotations = np.array(annotations.split(\" \")).astype(int)\n",
    "    image = np.zeros(shape=(height * width))\n",
    "    for i in np.arange(0, len(annotations), 2):\n",
    "        begin = annotations[i]\n",
    "        length = annotations[i+1] - 1\n",
    "        image[begin: begin+length] = 1\n",
    "    return image.reshape(height, width)\n",
    "\n",
    "class CellsAnnDataSet(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, color_transform=None, shape_transform=None):\n",
    "        self.df = None\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.df = process_df(df)\n",
    "        self.root_dir = root_dir\n",
    "        self.color_transform = color_transform\n",
    "        self.shape_transform = shape_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        image_id = self.df.iloc[index].name\n",
    "        image_name = f\"{image_id}.png\"\n",
    "        image_path = os.path.join(self.root_dir, image_name)\n",
    "        image = plt.imread(image_path) / 255\n",
    "        width, height = image.shape[:2]\n",
    "        annotations = self.df.iloc[index]['annotation']\n",
    "        annotation = reconstruct_annotations_image(annotations, width, height)\n",
    "        sample = {'image': image, \"annotation\": annotation}\n",
    "        if self.color_transform is not None:\n",
    "            sample['image'] = self.color_transform(sample['image'])\n",
    "        if self.shape_transform is not None:\n",
    "            sample = self.shape_transform(sample)\n",
    "        return sample\n",
    "\n",
    "class CellsImagesOnlyDataSet(Dataset):\n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        self.images = glob.glob(root_dir + \"/*.png\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        image = plt.imread(self.images[index]) / 255\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assertSizeArgValid(arg, is_four_tuple_valid=False):\n",
    "    assert isinstance(arg, (int, tuple))\n",
    "    if isinstance(arg, int):\n",
    "        if not is_four_tuple_valid:\n",
    "            return arg,arg\n",
    "        return arg,arg,arg,arg\n",
    "    else:\n",
    "        assert len(arg) == 2 or (len(arg) == 4 and is_four_tuple_valid)\n",
    "        if len(arg) == 2:\n",
    "            if not is_four_tuple_valid:\n",
    "                return arg\n",
    "            return arg + arg\n",
    "        else:\n",
    "            return arg\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = assertSizeArgValid(output_size)\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if not isinstance(sample, dict):\n",
    "            return transforms.RandomCrop(self.output_size)(sample)\n",
    "        image, annotation = sample['image'], sample['annotation']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        annotation = annotation[top: top + new_h,\n",
    "                                left: left + new_w]\n",
    "        return {'image': image, 'annotation': annotation}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if not isinstance(sample, dict):\n",
    "            return transforms.ToTensor()(sample[:,:,np.newaxis])\n",
    "        image, annotation = sample['image'], sample['annotation']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        return {'image': transforms.ToTensor()(image[:,:,np.newaxis]),\n",
    "                'annotation': transforms.ToTensor()(annotation[:,:,np.newaxis])}\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    def __call__(self, sample):\n",
    "        if not isinstance(sample, dict):\n",
    "            return transforms.RandomHorizontalFlip(self.p)(sample)\n",
    "        image, annotation = sample['image'], sample['annotation']\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = transforms.RandomHorizontalFlip(1)(image)\n",
    "            annotation = transforms.RandomHorizontalFlip(1)(annotation)\n",
    "        return {'image': image, 'annotation': annotation}\n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        assert isinstance(p, (int, float))\n",
    "        assert 0 <= p <= 1\n",
    "        self.p = p\n",
    "    def __call__(self, sample):\n",
    "        if not isinstance(sample, dict):\n",
    "            return transforms.RandomVerticalFlip(self.p)(sample)\n",
    "        image, annotation = sample['image'], sample['annotation']\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = transforms.RandomVerticalFlip(1)(image)\n",
    "            annotation = transforms.RandomVerticalFlip(1)(annotation)\n",
    "        return {'image': image, 'annotation': annotation}\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std, inplace=False):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if not isinstance(sample, dict):\n",
    "            return transforms.Normalize(self.mean, self.std, self.inplace)(sample)\n",
    "        image, annotation = sample['image'], sample['annotation']\n",
    "        image = transforms.Normalize(self.mean, self.std, self.inplace)(image)\n",
    "        annotation = transforms.Normalize(self.mean, self.std, self.inplace)(annotation)\n",
    "        return {'image': image, 'annotation': annotation}\n",
    "\n",
    "class Pad(object):\n",
    "    def __init__(self, padding, fill=0):\n",
    "        self.padding = assertSizeArgValid(padding, is_four_tuple_valid=True)\n",
    "        assert isinstance(fill, numbers.Number)\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if not isinstance(sample, dict):\n",
    "            return transforms.Pad(self.padding, fill=self.fill)(sample)\n",
    "        image, annotation = sample['image'], sample['annotation']\n",
    "        image = transforms.Pad(self.padding, fill=self.fill)(image)\n",
    "        annotation = transforms.Pad(self.padding, fill=self.fill)(annotation)\n",
    "        return {'image': image, 'annotation': annotation}\n",
    "\n",
    "class RandomPadToSize(object):\n",
    "    def __init__(self, output_size, fill=0):\n",
    "        self.output_size = assertSizeArgValid(output_size)\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        input_images, output_images = [], []\n",
    "        if not isinstance(sample, dict):\n",
    "            input_images = [sample.numpy().squeeze()]\n",
    "        else:\n",
    "            input_images = [sample['image'], sample['annotation']]\n",
    "        h, w = input_images[0].shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        assert new_w > w\n",
    "        assert new_h > h\n",
    "        top = np.random.randint(0, new_h - h)\n",
    "        left = np.random.randint(0, new_w - w)\n",
    "        for input_image in input_images:\n",
    "            output_image = np.full(self.output_size, self.fill)\n",
    "            output_image[top:top + h, left:left + w] = input_image\n",
    "            output_images.append(output_image)\n",
    "        if len(output_images) == 1:\n",
    "            return ToTensor()(output_images[1])\n",
    "        else:\n",
    "            return {'image': output_images[0], 'annotation': output_images[1]}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shape_transform = [\n",
    "    RandomCrop(np.random.randint(256, 520)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    RandomPadToSize((520, 704)),\n",
    "    ToTensor(),\n",
    "    Normalize(0.5, 0.5)\n",
    "]\n",
    "image_w_ann_data = CellsAnnDataSet(annot_root, train_csv,\n",
    "                                   shape_transform=transforms.Compose(shape_transform))\n",
    "image_only_data = CellsImagesOnlyDataSet(images_only_root,\n",
    "                                         transform=transforms.Compose([ToTensor()] + shape_transform))\n",
    "images_w_ann_loader = torch.utils.data.DataLoader(image_w_ann_data, batch_size = batch_size, shuffle=True)\n",
    "images_only_loader = torch.utils.data.DataLoader(image_only_data, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "sample_image, sample_ann = image_w_ann_data[0]['image'], image_w_ann_data[0]['annotation']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def formatForPlot(*args):\n",
    "    outputs = []\n",
    "    for arg in args:\n",
    "        if torch.is_tensor(arg):\n",
    "            arg =  arg.detach().cpu().numpy()\n",
    "        outputs.append(arg.squeeze())\n",
    "    return outputs if len(outputs) > 1 else outputs[0] if len(outputs) > 0 else None\n",
    "\n",
    "def plot_generator(orig_image, orig_ann, fake_ann):\n",
    "    orig_image, orig_ann, fake_ann = formatForPlot(orig_image, orig_ann, fake_ann)\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax = fig.subplots(ncols=3)\n",
    "    ax[0].imshow(orig_image.squeeze(), cmap='gray')\n",
    "    ax[1].imshow(orig_ann.squeeze(), cmap='gray')\n",
    "    ax[2].imshow(fake_ann.squeeze(), cmap='gray')\n",
    "    ax[0].set_title(\"image\")\n",
    "    ax[1].set_title(\"annotation\")\n",
    "    ax[1].set_title(\"fake annotation\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# draw some images:\n",
    "for i in range(5):\n",
    "    sample = image_w_ann_data[i]\n",
    "    image = sample['image']\n",
    "    annotation = sample['annotation']\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax = fig.subplots(ncols=2)\n",
    "    ax[0].imshow(formatForPlot(image), cmap='gray')\n",
    "    ax[1].imshow(formatForPlot(annotation), cmap='gray')\n",
    "    ax[0].set_title(\"image\")\n",
    "    ax[1].set_title(\"annotation\")\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_normal_weights(m, mean=0, std=0.02):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=mean, std=std)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=(4,4), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(4,4), stride=(2,2))\n",
    "        self.batchNorm2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4,4), stride=(2,2))\n",
    "        self.batchNorm3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=(4,4), stride=(2,2))\n",
    "        self.batchNorm4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=(4,4), stride=(2,2))\n",
    "        self.batchNorm5 = nn.BatchNorm2d(512)\n",
    "        self.conv6 = nn.Conv2d(512, 1, kernel_size=(4,4), stride=(2,2))\n",
    "        self.fc = nn.Linear(512, 1)\n",
    "        for conv in [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.conv6]:\n",
    "            init_normal_weights(conv)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z = torch.cat((x, y), dim=1)\n",
    "        z = F.leaky_relu(self.conv1(z), negative_slope=0.2, inplace=True)\n",
    "        z = F.leaky_relu(self.batchNorm2(self.conv2(z)), negative_slope=0.2, inplace=True)\n",
    "        z = F.leaky_relu(self.batchNorm3(self.conv3(z)), negative_slope=0.2, inplace=True)\n",
    "        z = F.leaky_relu(self.batchNorm4(self.conv4(z)), negative_slope=0.2, inplace=True)\n",
    "        z = F.leaky_relu(self.batchNorm5(self.conv5(z)), negative_slope=0.2, inplace=True)\n",
    "        z = self.conv6(z)\n",
    "        z = self.fc(z)\n",
    "        return torch.sigmoid(z)\n",
    "\n",
    "kernel_size = (5,5)\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, batchNorm=True):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=(2,2), padding='valid')]\n",
    "        if batchNorm:\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "        layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
    "        init_normal_weights(layers[0])\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout = True, output_padding=(0,0)):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=kernel_size, stride=(2,2),\n",
    "                               output_padding=output_padding),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        init_normal_weights(layers[0])\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.conv1on1 = nn.Conv2d(out_ch*2, out_ch, kernel_size=(1,1))\n",
    "\n",
    "    def forward(self, x, skip_in):\n",
    "        x = self.layers(x)\n",
    "        x = torch.cat((x, skip_in), dim=1)\n",
    "        return F.relu(self.conv1on1(x))\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.e1 = EncoderBlock(1, 64, batchNorm=False)\n",
    "        self.e2 = EncoderBlock(64, 128)\n",
    "        self.e3 = EncoderBlock(128, 256)\n",
    "        self.e4 = EncoderBlock(256, 512)\n",
    "        self.e5 = EncoderBlock(512, 512)\n",
    "        # self.e6 = EncoderBlock(512, 512)\n",
    "        # self.e7 = EncoderBlock(512, 512)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=kernel_size, stride=(2,2)),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        init_normal_weights(self.bottleneck[0])\n",
    "        # self.d1 = DecoderBlock(512, 512)\n",
    "        # self.d2 = DecoderBlock(512, 512)\n",
    "        self.d3 = DecoderBlock(512, 512)\n",
    "        self.d4 = DecoderBlock(512, 512, dropout=False)\n",
    "        self.d5 = DecoderBlock(512, 256, dropout=False, output_padding=(1,0))\n",
    "        self.d6 = DecoderBlock(256, 128, dropout=False)\n",
    "        self.d7 = DecoderBlock(128, 64, dropout=False, output_padding=(1,1))\n",
    "        self.output = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=kernel_size, stride=(2,2),\n",
    "                               output_padding=(1,1)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        init_normal_weights(self.output[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding:\n",
    "        print(f\"x.size() = {x.size()}\")\n",
    "        e1 = self.e1(x)\n",
    "        print(f\"e1.size() = {e1.size()}\")\n",
    "        e2 = self.e2(e1)\n",
    "        print(f\"e2.size() = {e2.size()}\")\n",
    "        e3 = self.e3(e2)\n",
    "        print(f\"e3.size() = {e3.size()}\")\n",
    "        e4 = self.e4(e3)\n",
    "        print(f\"e4.size() = {e4.size()}\")\n",
    "        e5 = self.e5(e4)\n",
    "        print(f\"e5.size() = {e5.size()}\")\n",
    "        # e6 = self.e6(e5)\n",
    "        # e7 = self.e7(e6)\n",
    "\n",
    "        # bottleneck:\n",
    "        b = self.bottleneck(e5)\n",
    "        print(f\"b.size() = {b.size()}\")\n",
    "\n",
    "        # decoding:\n",
    "        # d1 = self.d1(b, e7)\n",
    "        # d2 = self.d2(d1, e6)\n",
    "        d3 = self.d3(b, e5)\n",
    "        print(f\"d3.size() = {d3.size()}\")\n",
    "        d4 = self.d4(d3, e4)\n",
    "        print(f\"d4.size() = {d4.size()}\")\n",
    "        d5 = self.d5(d4, e3)\n",
    "        print(f\"d5.size() = {d5.size()}\")\n",
    "        d6 = self.d6(d5, e2)\n",
    "        print(f\"d6.size() = {d6.size()}\")\n",
    "        d7 = self.d7(d6, e1)\n",
    "        print(f\"d7.size() = {d7.size()}\")\n",
    "        return self.output(d7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CellGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CellGAN, self).__init__()\n",
    "        self.D = Discriminator()\n",
    "        self.G = Generator()\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.G.eval()\n",
    "        return self.G(x)\n",
    "\n",
    "\n",
    "    def G_loss(self, image):\n",
    "        self.G.train()\n",
    "        self.D.eval()\n",
    "        fake_ann = self.G(image)\n",
    "        D_fake = self.D(image, fake_ann)\n",
    "        return -torch.mean(torch.log(D_fake + epsilon))\n",
    "\n",
    "    def D_loss(self, image, real_ann = None):\n",
    "        self.D.train()\n",
    "        self.G.eval()\n",
    "        fake_ann = self.G(image)\n",
    "        D_fake = self.D(image, fake_ann)\n",
    "        if real_ann is not None:\n",
    "            D_real = self.D(image, real_ann)\n",
    "            return -torch.mean(torch.log(D_real + epsilon) + torch.log(1-D_fake + epsilon))\n",
    "        else:\n",
    "            return -torch.mean(torch.log(1-D_fake + epsilon))\n",
    "\n",
    "    def run_training(self,epochs: int = 10, G_lr: int = 2e-4, D_lr: int = 2e-4, plot:bool = True):\n",
    "        D_Opt = torch.optim.Adam(params=self.D.parameters(), lr=D_lr, betas=(0.5,0.5))\n",
    "        G_Opt = torch.optim.Adam(params=self.G.parameters(), lr=G_lr, betas=(0.5,0.5))\n",
    "        d_losses, g_losses = [], []\n",
    "        for epoch in range(1, epochs+1):\n",
    "            images_only_iter = iter(images_only_loader)\n",
    "            d_epoch_losses, g_epoch_losses = [], []\n",
    "            for i, batch in enumerate(images_w_ann_loader, 0):\n",
    "                image, ann = batch['image'].to(device), batch['annotation'].to(device)\n",
    "                d_step_loss, g_step_loss = 0, 0\n",
    "                # train discriminator:\n",
    "                d_loss = self.D_loss(image, ann)\n",
    "                D_Opt.zero_grad()\n",
    "                d_loss.backward()\n",
    "                D_Opt.step()\n",
    "                d_step_loss += d_loss.item() / 2\n",
    "\n",
    "                # train generator:\n",
    "                g_loss = self.G_loss(image)\n",
    "                G_Opt.zero_grad()\n",
    "                g_loss.backward()\n",
    "                G_Opt.step()\n",
    "                g_step_loss += g_loss.item() / 2\n",
    "\n",
    "                # semi supervised:\n",
    "                for _ in range(3):\n",
    "                    step_d_losses, step_g_losses = [],[]\n",
    "                    image = next(images_only_iter)\n",
    "\n",
    "                    # train discriminator:\n",
    "                    d_loss = self.D_loss(image)\n",
    "                    D_Opt.zero_grad()\n",
    "                    d_loss.backward()\n",
    "                    D_Opt.step()\n",
    "                    g_step_loss += d_loss.item() / 6\n",
    "\n",
    "                    # train generator:\n",
    "                    g_loss = self.G_loss(image)\n",
    "                    G_Opt.zero_grad()\n",
    "                    g_loss.backward()\n",
    "                    G_Opt.step()\n",
    "                    g_step_loss += g_loss.item() / 6\n",
    "                d_epoch_losses.append(d_step_loss)\n",
    "                g_epoch_losses.append(g_step_loss)\n",
    "\n",
    "            d_losses.append(np.mean(d_epoch_losses))\n",
    "            g_losses.append(np.mean(g_epoch_losses))\n",
    "            print(f'epoch [{epoch}/{epochs}],'\n",
    "                  f' generator loss:{g_losses[epoch-1].round(3)},'\n",
    "                  f' discriminator loss:{d_losses[epoch-1].round(3)}')\n",
    "            if plot:\n",
    "                self.G.eval()\n",
    "                with torch.no_grad:\n",
    "                    fake_ann = self.G(sample_image)\n",
    "                    plot_generator(sample_image, sample_ann, fake_ann)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}